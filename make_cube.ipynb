{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f725232",
   "metadata": {},
   "source": [
    "# make_cube\n",
    "\n",
    "This notebook contains the code necessary for creating a data cube containing all line parameters, RVs, and measured activity indices given a list of lines and a list of NEID files. This notebook does not include the line list and files themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f90c2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "from numpy.polynomial import polynomial as P\n",
    "from scipy import interpolate\n",
    "from scipy.constants import c\n",
    "from scipy.optimize import leastsq\n",
    "from scipy import stats\n",
    "from scipy.optimize import curve_fit\n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "\n",
    "#model fitting\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.interpolate import CubicSpline\n",
    "from astropy.modeling import models, fitting\n",
    "from astropy import modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed6762c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunked_continuum_fit function written by Arpita Roy\n",
    "\n",
    "#this function divides the spectrum into \"chunks\", take the maximum value of each chunk, and fits a polynomial \n",
    "#over these chunks. the spectrum is then normalized by dividing out this estimate of the continuum\n",
    "def chunked_continuum_fit(x, y, percentile_cut=95.0, nchunks=15, deg=5):\n",
    "    chunks_x = np.zeros(nchunks) #chunk up spectrum and store mini-spectrum chunks in lists\n",
    "    chunks_y = np.zeros(nchunks)\n",
    "    npixels = len(x)\n",
    "    pixels_per_chunk = npixels / nchunks\n",
    "    for i in range(nchunks): #loop through each chunk\n",
    "        chunk_i1 = int(i*pixels_per_chunk)\n",
    "        chunk_i2 = int((i+1)*pixels_per_chunk)\n",
    "        if chunk_i2 > npixels: #chop end of last chunk\n",
    "            chunk_i2 = npixels\n",
    "        chunks_y[i] = np.nanpercentile(y[chunk_i1:chunk_i2], percentile_cut) #chunk y is xxth percentile of chunk\n",
    "        chunks_x[i] = 0.5*(x[chunk_i1:chunk_i2][0]+x[chunk_i1:chunk_i2][-1]) #chunk x is just center of chunk\n",
    "    smooth_y = savgol_filter(chunks_y, 11, 1) #add some smoothin to avoid \"falling\" into lines\n",
    "    pfit = np.polyfit(chunks_x, smooth_y, deg=deg) #fit polynomial to chunks\n",
    "    cs = CubicSpline(chunks_x, chunks_y)\n",
    "    return np.polyval(pfit, x), cs, chunks_x, chunks_y #return continuum array generated from polynomial fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec2a11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple gaussian function\n",
    "def gauss(x, a, x0, sigma):\n",
    "    return a * np.exp(-(x - x0)**2 / (2 * sigma**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a0f8f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cube code written by Sarah Jiang, adapted from code written by Justin Otor\n",
    "def create_cube(file_list, line_list, cube_name='',\n",
    "                    lstart=0, lend=None):\n",
    "    \"\"\"\n",
    "    Create a data cube from a list of NEID FITS files.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    file_list : list, required\n",
    "        A list of file paths leading to FITS files observational data from NEID\n",
    "        \n",
    "    line_list : numpy array or `~xarray.core.dataset.Dataset`, required\n",
    "        A numpy array or xarray Dataset with information on spectral lines of interest.\n",
    "         \n",
    "    cube_name : str, optional\n",
    "        The name of the DataArray that will hold the data cube.\n",
    "        \n",
    "    lstart : int, optional\n",
    "        The first index in `line_list` to be considered for the data cube.\n",
    "        [default: 0]\n",
    "        \n",
    "    lend : int, optional\n",
    "        The last index in `line_list` to be considered for the data cube. Lines\n",
    "        from `lstart` to `lend` will be included in the final product. An `lend`\n",
    "        of None equates including all lines from `lstart` onward.\n",
    "        [default: None]\n",
    "\n",
    "    \"\"\"\n",
    "    #set up parameter arrays\n",
    "    nlines = len(line_list)\n",
    "    nfiles = len(file_list)\n",
    "    \n",
    "    #initialize all parameter arrays with zeros in the dimensions of nfiles x nlines\n",
    "    #these arrays will store the value of each parameter (e.g. centroid) for each file in file_list \n",
    "    #and each line in line_list, and errors\n",
    "    #LINE PARAMETERS\n",
    "    centroids = np.zeros((nfiles, nlines))\n",
    "    centroid_errs = np.zeros(centroids.shape)\n",
    "    depths = np.zeros(centroids.shape)\n",
    "    depth_errs = np.zeros(centroids.shape)\n",
    "    fwhms = np.zeros(centroids.shape)\n",
    "    fwhm_errs = np.zeros(centroids.shape)\n",
    "    intfluxs = np.zeros(centroids.shape)\n",
    "    intflux_errs = np.zeros(centroids.shape)\n",
    "    \n",
    "    #RV AND ACTIVITY INDICATORS\n",
    "    rvs = np.zeros(centroids.shape)\n",
    "    rv_errs = np.zeros(centroids.shape)\n",
    "    cahks = np.zeros(centroids.shape)\n",
    "    cahk_errs = np.zeros(centroids.shape)\n",
    "    hei_1s = np.zeros(centroids.shape)\n",
    "    hei_1_errs = np.zeros(centroids.shape)\n",
    "    hei_2s = np.zeros(centroids.shape)\n",
    "    hei_2_errs = np.zeros(centroids.shape)\n",
    "    nais = np.zeros(centroids.shape)\n",
    "    nai_errs = np.zeros(centroids.shape)\n",
    "    ha06_1s = np.zeros(centroids.shape)\n",
    "    ha06_1_errs = np.zeros(centroids.shape)\n",
    "    ha06_2s = np.zeros(centroids.shape)\n",
    "    ha06_2_errs = np.zeros(centroids.shape)\n",
    "    ha16_1s = np.zeros(centroids.shape)\n",
    "    ha16_1_errs = np.zeros(centroids.shape)\n",
    "    ha16_2s = np.zeros(centroids.shape)\n",
    "    ha16_2_errs = np.zeros(centroids.shape)\n",
    "    cai_1s = np.zeros(centroids.shape)\n",
    "    cai_1_errs = np.zeros(centroids.shape)\n",
    "    cai_2s = np.zeros(centroids.shape)\n",
    "    cai_2_errs = np.zeros(centroids.shape)\n",
    "    cairt1s = np.zeros(centroids.shape)\n",
    "    cairt1_errs = np.zeros(centroids.shape)\n",
    "    cairt2s = np.zeros(centroids.shape)\n",
    "    cairt2_errs = np.zeros(centroids.shape)\n",
    "    cairt3s = np.zeros(centroids.shape)\n",
    "    cairt3_errs = np.zeros(centroids.shape)\n",
    "    nainirs = np.zeros(centroids.shape)\n",
    "    nainir_errs = np.zeros(centroids.shape)\n",
    "    padeltas = np.zeros(centroids.shape)\n",
    "    padelta_errs = np.zeros(centroids.shape)\n",
    "    \n",
    "    #initialize empty arrays to store timestamps of each file in file_list and wavelengths of each line in line_list\n",
    "    times = []\n",
    "    lines = []\n",
    "    \n",
    "    #iterate through line_list\n",
    "    for l, line in tqdm(enumerate(line_list)):\n",
    "        #collect line wavelength\n",
    "        lines.append(line)\n",
    "        \n",
    "        #initialize arrays to hold data for this line over all files in file_list\n",
    "        line_waves = []\n",
    "        line_specs = []\n",
    "        line_errs = []\n",
    "        \n",
    "        #if any observations have NaNs in the window of the line, the gaussian fit will fail and \n",
    "        #flag corr_err = True\n",
    "        #initialize corr_err = False\n",
    "        corr_err = False\n",
    "        \n",
    "        #iterate through file_list\n",
    "        for o, file in enumerate(file_list):\n",
    "            #fetch data and header info from file\n",
    "            hdul = fits.open(file)\n",
    "            if l == 0: #collect file timestamp; only do this once\n",
    "                times.append(hdul['PRIMARY'].header['OBSJD'])\n",
    "\n",
    "            flux = hdul['SCIFLUX'].data\n",
    "            wave = hdul['SCIWAVE'].data\n",
    "            blaze = hdul['SCIBLAZE'].data\n",
    "            err = hdul['SCIVAR'].data\n",
    "                  \n",
    "            #collect rv and act idxs and errors from NEID file and place into the correct file x line position in \n",
    "            #the parameter arrays\n",
    "            rvs[o, lstart + l] = hdul['CCFS'].header['CCFRVMOD']\n",
    "            rv_errs[o, lstart + l] = hdul['CCFS'].header['DVRMSMOD']\n",
    "            cahks[o, lstart + l] = hdul['ACTIVITY'].data[0][1]\n",
    "            cahk_errs[o, lstart + l] = hdul['ACTIVITY'].data[0][2]\n",
    "            hei_1s[o, lstart + l] = hdul['ACTIVITY'].data[1][1]\n",
    "            hei_1_errs[o, lstart + l] = hdul['ACTIVITY'].data[1][2]\n",
    "            hei_2s[o, lstart + l] = hdul['ACTIVITY'].data[2][1]\n",
    "            hei_2_errs[o, lstart + l] = hdul['ACTIVITY'].data[2][2]\n",
    "            nais[o, lstart + l] = hdul['ACTIVITY'].data[3][1]\n",
    "            nai_errs[o, lstart + l] = hdul['ACTIVITY'].data[3][2]\n",
    "            ha06_1s[o, lstart + l] = hdul['ACTIVITY'].data[4][1]\n",
    "            ha06_1_errs[o, lstart + l] = hdul['ACTIVITY'].data[4][2]\n",
    "            ha06_2s[o, lstart + l] = hdul['ACTIVITY'].data[5][1]\n",
    "            ha06_2_errs[o, lstart + l] = hdul['ACTIVITY'].data[5][2]\n",
    "            ha16_1s[o, lstart + l] = hdul['ACTIVITY'].data[6][1]\n",
    "            ha16_1_errs[o, lstart + l] = hdul['ACTIVITY'].data[6][2]\n",
    "            ha16_2s[o, lstart + l] = hdul['ACTIVITY'].data[7][1]\n",
    "            ha16_2_errs[o, lstart + l] = hdul['ACTIVITY'].data[7][2]\n",
    "            cai_1s[o, lstart + l] = hdul['ACTIVITY'].data[8][1]\n",
    "            cai_1_errs[o, lstart + l] = hdul['ACTIVITY'].data[8][2]\n",
    "            cai_2s[o, lstart + l] = hdul['ACTIVITY'].data[9][1]\n",
    "            cai_2_errs[o, lstart + l] = hdul['ACTIVITY'].data[9][2]\n",
    "            cairt1s[o, lstart + l] = hdul['ACTIVITY'].data[10][1]\n",
    "            cairt1_errs[o, lstart + l] = hdul['ACTIVITY'].data[10][2]\n",
    "            cairt2s[o, lstart + l] = hdul['ACTIVITY'].data[11][1]\n",
    "            cairt2_errs[o, lstart + l] = hdul['ACTIVITY'].data[11][2]\n",
    "            cairt3s[o, lstart + l] = hdul['ACTIVITY'].data[12][1]\n",
    "            cairt3_errs[o, lstart + l] = hdul['ACTIVITY'].data[12][2]\n",
    "            nainirs[o, lstart + l] = hdul['ACTIVITY'].data[13][1]\n",
    "            nainir_errs[o, lstart + l] = hdul['ACTIVITY'].data[13][2]\n",
    "            padeltas[o, lstart + l] = hdul['ACTIVITY'].data[14][1]\n",
    "            padelta_errs[o, lstart + l] = hdul['ACTIVITY'].data[14][2]\n",
    "\n",
    "            #handle NaNs\n",
    "            flux[np.isnan(flux)] = 0\n",
    "            wave[np.isnan(wave)] = 0\n",
    "            err[np.isnan(err)] = 0\n",
    "\n",
    "            #get NEID order for line\n",
    "            #lines can appear in multiple orders since the wavelength range of each order overlap; get all orders\n",
    "            #that exhibit line and choose the one where the line is closest to the middle of the order\n",
    "            locs = np.apply_along_axis(np.searchsorted, 1, wave, line)\n",
    "            matches = (locs > 0) * (locs < wave.shape[1])\n",
    "\n",
    "            orders, = np.where(matches==True)\n",
    "            if len(orders) == 1:\n",
    "                line_order = orders[0] #only one order\n",
    "            elif len(orders) == 3:\n",
    "                line_order = orders[1] #choose middle order\n",
    "            else: #find order where line is closest to the middle of the order\n",
    "                order1 = orders[0]\n",
    "                order2 = orders[1]\n",
    "\n",
    "                order1_mean = (wave[order1,0]+wave[order1,9215])/2\n",
    "                order2_mean = (wave[order2,0]+wave[order2,9215])/2\n",
    "\n",
    "                diff1 = abs(line-order1_mean)\n",
    "                diff2 = abs(line-order2_mean)\n",
    "\n",
    "                if diff1 < diff2:\n",
    "                    line_order = orders[0]\n",
    "                else:\n",
    "                    line_order = orders[1]\n",
    "\n",
    "            #shift wavelength axis to the rest frame of the star\n",
    "            barycorr = hdul['PRIMARY'].header['SSBRV'+str(line_order+52).zfill(3)] #add 52 to get header order\n",
    "            rv = hdul['CCFS'].header['CCFRVMOD'] #measured rv #km/s\n",
    "            delta_rv = rv-barycorr #km/s\n",
    "\n",
    "            delta_rv = delta_rv*1000 #convert to m/s\n",
    "            avg_wvl = (wave[line_order,0]+wave[line_order,9215])/2\n",
    "            f = c/avg_wvl #divide by average wavelength of order\n",
    "            delta_lambda = delta_rv/f\n",
    "\n",
    "            #collect shifted wavelength axes\n",
    "            line_wave = wave[line_order,:]-delta_lambda\n",
    "            line_waves.append(line_wave)\n",
    "\n",
    "            #collect blaze-corrected spectra\n",
    "            line_spec = flux[line_order,:]/blaze[line_order,:]\n",
    "            line_specs.append(line_spec)\n",
    "            \n",
    "            #collect errors on blaze-corrected spectra\n",
    "            line_err = err[line_order,:]/blaze[line_order,:]\n",
    "            line_errs.append(line_err)\n",
    "        \n",
    "        line_waves = np.vstack(line_waves)\n",
    "        line_specs = np.vstack(line_specs)\n",
    "        line_errs = np.vstack(line_errs)\n",
    "                        \n",
    "        #normalize spectra and errors using chunkfit function\n",
    "        line_specs_norm = []\n",
    "        line_errs_norm = []\n",
    "\n",
    "        for i, wave in enumerate(line_waves):\n",
    "            chunkfit, cs,chunks_x, chunks_y = chunked_continuum_fit(wave,line_specs[i],percentile_cut=90.0, nchunks=100, deg=1)\n",
    "            line_specs_norm.append(line_specs[i]/chunkfit)\n",
    "            line_errs_norm.append(line_errs[i]/chunkfit)\n",
    "\n",
    "        line_specs_norm = np.vstack(line_specs_norm)\n",
    "        line_errs_norm = np.vstack(line_errs_norm)\n",
    "        \n",
    "        #calculate integrated flux using uniform velocity window of 3 km/s and interpolating within window\n",
    "        line_fluxs = []\n",
    "        flux_errs = []\n",
    "        \n",
    "        for i, wave in enumerate(line_waves):\n",
    "            #convert velocity window to corresponding wavelength window\n",
    "            rv_wing = 3000 #m/s\n",
    "            f = c/line\n",
    "            lambda_wing = rv_wing/f\n",
    "\n",
    "            #generate uniform wavelength window\n",
    "            flux_start = line-lambda_wing\n",
    "            flux_stop = line+lambda_wing\n",
    "\n",
    "            flux_range = np.linspace(flux_start, flux_stop, 50)\n",
    "\n",
    "            #interpolate flux over uniform window\n",
    "            x = wave\n",
    "            y = line_specs_norm[i]\n",
    "            f = interpolate.interp1d(x,y,kind='cubic')\n",
    "            \n",
    "            interp_flux = f(flux_range)\n",
    "            #sum flux to get integrated flux\n",
    "            int_flux = np.nansum(interp_flux)\n",
    "            \n",
    "            #calculate error on integrated flux\n",
    "            flux_plus_error = y + line_errs_norm[i]\n",
    "            fplus = interpolate.interp1d(x, flux_plus_error, kind='cubic')\n",
    "            interp_plus = fplus(flux_range)\n",
    "            errs = interp_plus-interp_flux\n",
    "            \n",
    "            sqrd_errs = [err**2 for err in errs]\n",
    "            flux_err = np.sqrt(np.nansum(sqrd_errs))\n",
    "            \n",
    "            #collect integrated fluxes and errors\n",
    "            line_fluxs.append(int_flux)\n",
    "            flux_errs.append(flux_err)\n",
    "\n",
    "        #now clip spectra to line, gaussian fit line, and get other line parameters (centroid, depth, FWHM)\n",
    "        clipped_waves = []\n",
    "        clipped_specs = []\n",
    "        clipped_errs = []\n",
    "\n",
    "        #assume line width of 0.4A (0.2A on either side) and use these as starting bounds for line\n",
    "        wing = 0.2\n",
    "\n",
    "        line_start = line-wing\n",
    "        line_stop = line+wing\n",
    "\n",
    "        for i, wave in enumerate(line_waves):\n",
    "            #find closest wavelength value to 0.2A wings\n",
    "            start_diff_list = wave - line_start\n",
    "            stop_diff_list = wave - line_stop\n",
    "\n",
    "            start_diff_list = [abs(i) for i in start_diff_list]\n",
    "            stop_diff_list = [abs(i) for i in stop_diff_list]\n",
    "\n",
    "            closest_start = np.nanmin(start_diff_list)\n",
    "            closest_stop = np.nanmin(stop_diff_list)\n",
    "\n",
    "            start_idx, = np.where(start_diff_list==closest_start)\n",
    "            stop_idx, = np.where(stop_diff_list==closest_stop)\n",
    "\n",
    "            #clip wavelength axes, spectra, and spectra error arrays to 0.2A wings\n",
    "            clipped_wave = line_waves[i][int(start_idx):int(stop_idx)]\n",
    "            clipped_spec = line_specs_norm[i][int(start_idx):int(stop_idx)]\n",
    "            clipped_err = line_errs_norm[i][int(start_idx):int(stop_idx)]\n",
    "\n",
    "            clipped_waves.append(clipped_wave)\n",
    "            clipped_specs.append(clipped_spec)\n",
    "            clipped_errs.append(clipped_err)\n",
    "\n",
    "        line_peaks = []\n",
    "        line_centroids = []\n",
    "        line_fwhms = []\n",
    "        \n",
    "        line_peak_errs = []\n",
    "        line_centroid_errs = []\n",
    "        line_fwhm_errs = []\n",
    "        \n",
    "        #---------------------------------------START OF GAUSSIAN FITTING\n",
    "\n",
    "        #iterate through clipped data to fit for parameters in each observation\n",
    "        for i in range(len(clipped_waves)):\n",
    "            x = clipped_waves[i]\n",
    "            y = -clipped_specs[i]+1 #flip spectra to enable gaussian fitting\n",
    "            yerr = clipped_errs[i]\n",
    "\n",
    "            if 0 not in clipped_waves[i] and 0 not in clipped_specs[i]: #throw out obs that have NaNs/0s\n",
    "                #fit gaussian; if this function fails, flag corr_err = True\n",
    "                try:\n",
    "                    popt, cov = curve_fit(gauss, x, y, p0=[0.5, line, 0.2], sigma=yerr,\n",
    "                                          bounds=[[0,line-1,0],[1,line+1,1]])\n",
    "                except:\n",
    "                    corr_err = True\n",
    "                    continue\n",
    "                    \n",
    "                #get line depth/peak\n",
    "                peak = popt[0]\n",
    "\n",
    "                #get centroid\n",
    "                centroid = popt[1]\n",
    "\n",
    "                #get fwhm\n",
    "                fwhm = 2*popt[2]*np.sqrt(2*np.log(2))\n",
    "                \n",
    "                #get errors of gaussian fit; if this fails, flag corr_err = True\n",
    "                try:\n",
    "                    perr = np.sqrt(np.diag(cov))\n",
    "                except:\n",
    "                    perr = np.zeros(3)\n",
    "\n",
    "                #get parameter errors\n",
    "                depth_err = perr[0]\n",
    "                centroid_err = perr[1]\n",
    "                fwhm_err = 2*perr[2]*np.sqrt(2*np.log(2))\n",
    "                \n",
    "                #collect parameters and errors\n",
    "                line_peak_errs.append(depth_err)\n",
    "                line_centroid_errs.append(centroid_err)\n",
    "                line_fwhm_errs.append(fwhm_err)\n",
    "                \n",
    "                line_peaks.append(peak)\n",
    "                line_centroids.append(centroid)\n",
    "                line_fwhms.append(fwhm)\n",
    "            else:\n",
    "                corr_err = True\n",
    "\n",
    "        #if any of the steps that flag corr_err occur, skip/discard line\n",
    "        if corr_err == True:\n",
    "            print('ERROR: At least one file has NANs in window')\n",
    "            continue\n",
    "        \n",
    "        #now check that the fit was accurate by checking if the mean centroid measured for this line is \n",
    "        #within 0.01A of the known line center\n",
    "        line_centroids = np.array(line_centroids)\n",
    "        mean_centroid = np.nanmean(line_centroids)\n",
    "\n",
    "        std = 0.2\n",
    "        counter = 0\n",
    "\n",
    "        try:\n",
    "            #iterate through 19 steps of narrowing the line window by 0.01 and re-fitting the gaussian until the\n",
    "            #mean centroid is within 0.01A of the known line center\n",
    "            while abs(mean_centroid-line) > 0.01 and counter < 18:\n",
    "                wing -= 0.01\n",
    "                std -= 0.01\n",
    "\n",
    "                clipped_waves = []\n",
    "                clipped_specs = []\n",
    "                clipped_errs = []\n",
    "\n",
    "                line_start = line-wing\n",
    "                line_stop = line+wing\n",
    "\n",
    "                for i, wave in enumerate(line_waves):\n",
    "                    start_diff_list = wave - line_start\n",
    "                    stop_diff_list = wave - line_stop\n",
    "\n",
    "                    start_diff_list = [abs(i) for i in start_diff_list]\n",
    "                    stop_diff_list = [abs(i) for i in stop_diff_list]\n",
    "\n",
    "                    closest_start = np.nanmin(start_diff_list)\n",
    "                    closest_stop = np.nanmin(stop_diff_list)\n",
    "\n",
    "                    start_idx, = np.where(start_diff_list==closest_start)\n",
    "                    stop_idx, = np.where(stop_diff_list==closest_stop)\n",
    "\n",
    "                    clipped_wave = line_waves[i][int(start_idx):int(stop_idx)]\n",
    "                    clipped_spec = line_specs_norm[i][int(start_idx):int(stop_idx)]\n",
    "                    clipped_err = line_errs_norm[i][int(start_idx):int(stop_idx)]\n",
    "\n",
    "                    clipped_waves.append(clipped_wave)\n",
    "                    clipped_specs.append(clipped_spec)\n",
    "                    clipped_errs.append(clipped_err)\n",
    "\n",
    "                line_peaks = []\n",
    "                line_centroids = []\n",
    "                line_fwhms = []\n",
    "                \n",
    "                line_peak_errs = []\n",
    "                line_centroid_errs = []\n",
    "                line_fwhm_errs = []\n",
    "\n",
    "                for i in range(len(clipped_waves)):\n",
    "                    x = clipped_waves[i]\n",
    "                    y = -clipped_specs[i]+1\n",
    "                    yerr = clipped_errs[i]\n",
    "\n",
    "                    if 0 not in clipped_waves[i] and 0 not in clipped_specs[i]: #throw out obs that have nans/0s\n",
    "                        try:\n",
    "                            popt, cov = curve_fit(gauss, x, y, p0=[0.5, line, std], sigma=yerr,\n",
    "                                                  bounds=[[0,line-1,0],[1,line+1,1]])\n",
    "                        except:\n",
    "                            corr_err == True\n",
    "                            continue\n",
    "\n",
    "                        #get line depth/peak\n",
    "                        peak = popt[0]\n",
    "\n",
    "                        #get centroid\n",
    "                        centroid = popt[1]\n",
    "\n",
    "                        #get fwhm\n",
    "                        fwhm = 2*popt[2]*np.sqrt(2*np.log(2))\n",
    "                        \n",
    "                        try:\n",
    "                            perr = np.sqrt(np.diag(cov))\n",
    "                        except:\n",
    "                            perr = np.zeros(3)\n",
    "                            \n",
    "                        depth_err = perr[0]\n",
    "                        centroid_err = perr[1]\n",
    "                        fwhm_err = 2*perr[2]*np.sqrt(2*np.log(2))\n",
    "\n",
    "                        line_peak_errs.append(depth_err)\n",
    "                        line_centroid_errs.append(centroid_err)\n",
    "                        line_fwhm_errs.append(fwhm_err)\n",
    "\n",
    "                        line_peaks.append(peak)\n",
    "                        line_centroids.append(centroid)   \n",
    "                        line_fwhms.append(fwhm)\n",
    "\n",
    "                if corr_err == True:\n",
    "                    continue\n",
    "                        \n",
    "                line_centroids = np.array(line_centroids)\n",
    "                mean_centroid = np.nanmean(line_centroids)\n",
    "                counter += 1\n",
    "        except: #if iterative narrowing and re-fitting fails during the process, skip/discard line\n",
    "            continue\n",
    "\n",
    "        #if iterative narrowing and re-fitting finishes but the mean centroid is still not within 0.01A of the\n",
    "        #known line center, skip/discard line\n",
    "        if counter == 18:\n",
    "            continue\n",
    "        \n",
    "        #at this point, lines have been filtered for generally \"clean\" lines without NaNs that can be accurately \n",
    "        #fitted with a standard gaussian within +/- 0.2A\n",
    "        \n",
    "        #collect centroids, fwhms, depths, and integrated fluxes and errors for ALL observations for line\n",
    "        #and place into the correct files x line positions in the parameter arrays\n",
    "        centroids[:len(line_centroids), lstart + l] = line_centroids\n",
    "        centroid_errs[:len(line_centroid_errs), lstart + l] = line_centroid_errs\n",
    "        fwhms[:len(line_fwhms), lstart + l] = line_fwhms\n",
    "        fwhm_errs[:len(line_fwhm_errs), lstart + l] = line_fwhm_errs\n",
    "        depths[:len(line_peaks), lstart + l] = line_peaks\n",
    "        depth_errs[:len(line_peak_errs), lstart + l] = line_peak_errs\n",
    "        intfluxs[:len(line_fluxs), lstart + l] = line_fluxs\n",
    "        intflux_errs[:len(flux_errs), lstart + l] = flux_errs\n",
    "                    \n",
    "    #create the DataArray after joining parameter arrays, swapping line and time axes so time comes last\n",
    "    join_params = np.array([centroids, centroid_errs, depths, depth_errs, fwhms, fwhm_errs, intfluxs, intflux_errs,\n",
    "                            rvs, rv_errs, cahks, cahk_errs, hei_1s, hei_1_errs, hei_2s, hei_2_errs, nais, nai_errs,\n",
    "                            ha06_1s, ha06_1_errs, ha06_2s, ha06_2_errs, ha16_1s, ha16_1_errs, ha16_2s, ha16_2_errs,\n",
    "                            cai_1s, cai_1_errs, cai_2s, cai_2_errs, cairt1s, cairt1_errs, cairt2s, cairt2_errs,\n",
    "                            cairt3s, cairt3_errs, nainirs, nainir_errs, padeltas, padelta_errs]).swapaxes(1,2)\n",
    "    \n",
    "    cube = xr.DataArray(join_params,\n",
    "                             dims=('param', 'line', 'time'),\n",
    "                             coords={'param': ['centroid', 'centroid_err', 'depth', 'depth_err', 'fwhm', 'fwhm_err',\n",
    "                                               'int_flux', 'int_flux_err', 'rv', 'rv_err', 'cahk', 'cahk_err',\n",
    "                                               'hei_1', 'hei_1_err', 'hei_2', 'hei_2_err', 'nai', 'nai_err', 'ha06_1',\n",
    "                                               'ha06_1_err', 'ha06_2', 'ha06_2_err', 'ha16_1', 'ha16_1_err', 'ha16_2',\n",
    "                                               'ha16_2_err', 'cai_1', 'cai_1_err', 'cai_2', 'cai_2_err', 'cairt1',\n",
    "                                               'cairt1_err', 'cairt2', 'cairt2_err', 'cairt3', 'cairt3_err', 'nainir',\n",
    "                                               'nainir_err', 'padelta', 'padelta_err'],\n",
    "                                     'line': lines,\n",
    "                                     'time': times\n",
    "                                    },\n",
    "                             name=cube_name,\n",
    "                            )\n",
    "    \n",
    "    #add parameter-specific labels (for time)\n",
    "    cube.coords['time'].attrs['unit'] = 'JD'\n",
    "    \n",
    "    #return cube containing centroid, depth, fwhm, integrated flux, rvs, act idxs, and all errors, wavelength\n",
    "    #values of each line, and timestamps of each observation\n",
    "    return cube"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
